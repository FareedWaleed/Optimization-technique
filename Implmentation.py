# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1D7QTEqMEy0j-REax7yi1WBvx3YE0UFtQ

Implementation of Logistic Regression with Optimization Techniques

1:Import Libraries
"""

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix

"""2:Generate Synthetic Data"""

X, y = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0,
                           n_clusters_per_class=1, flip_y=0.05, random_state=42)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Add bias term
X_train = np.hstack((np.ones((X_train.shape[0], 1)), X_train))
X_test = np.hstack((np.ones((X_test.shape[0], 1)), X_test))

"""3:Implement Logistic Regression Class"""

class LogisticRegression:
    def __init__(self):
        self.weights = None

    def sigmoid(self, z):
        return 1 / (1 + np.exp(-z))

    def predict_proba(self, X):
        return self.sigmoid(np.dot(X, self.weights))

    def predict(self, X, threshold=0.5):
        return (self.predict_proba(X) >= threshold).astype(int)

    def binary_cross_entropy(self, y_true, y_pred):
        epsilon = 1e-15
        y_pred = np.clip(y_pred, epsilon, 1 - epsilon)
        return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))

    def compute_gradient(self, X, y, y_pred):
        return np.dot(X.T, (y_pred - y)) / len(y)

    def fit(self, X, y, optimizer='gd', learning_rate=0.01, epochs=1000, batch_size=32, verbose=False):
        n_samples, n_features = X.shape
        self.weights = np.zeros(n_features)
        losses = []

        if optimizer == 'gd':
            for epoch in range(epochs):
                y_pred = self.predict_proba(X)
                grad = self.compute_gradient(X, y, y_pred)
                self.weights -= learning_rate * grad
                loss = self.binary_cross_entropy(y, y_pred)
                losses.append(loss)
                if verbose and (epoch + 1) % 100 == 0:
                    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}")

        elif optimizer == 'bgd':
            for epoch in range(epochs):
                y_pred = self.predict_proba(X)
                grad = self.compute_gradient(X, y, y_pred)
                self.weights -= learning_rate * grad
                loss = self.binary_cross_entropy(y, y_pred)
                losses.append(loss)
                if verbose and (epoch + 1) % 100 == 0:
                    print(f"Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}")

        elif optimizer == 'sgd':
            for epoch in range(epochs):
                indices = np.random.permutation(n_samples)
                X_shuffled, y_shuffled = X[indices], y[indices]
                epoch_loss = 0.0
                for i in range(n_samples):
                    xi = X_shuffled[i:i+1]
                    yi = y_shuffled[i:i+1]
                    y_pred_i = self.predict_proba(xi)
                    grad = self.compute_gradient(xi, yi, y_pred_i)
                    self.weights -= learning_rate * grad
                    epoch_loss += self.binary_cross_entropy(yi, y_pred_i)
                losses.append(epoch_loss / n_samples)
                if verbose and (epoch + 1) % 100 == 0:
                    print(f"Epoch {epoch+1}/{epochs}, Loss: {losses[-1]:.4f}")

        elif optimizer == 'mbgd':
            for epoch in range(epochs):
                indices = np.random.permutation(n_samples)
                X_shuffled, y_shuffled = X[indices], y[indices]
                epoch_loss = 0.0
                num_batches = int(np.ceil(n_samples / batch_size))
                for i in range(num_batches):
                    start = i * batch_size
                    end = start + batch_size
                    X_batch = X_shuffled[start:end]
                    y_batch = y_shuffled[start:end]
                    y_pred_batch = self.predict_proba(X_batch)
                    grad = self.compute_gradient(X_batch, y_batch, y_pred_batch)
                    self.weights -= learning_rate * grad
                    epoch_loss += self.binary_cross_entropy(y_batch, y_pred_batch)
                losses.append(epoch_loss / num_batches)
                if verbose and (epoch + 1) % 100 == 0:
                    print(f"Epoch {epoch+1}/{epochs}, Loss: {losses[-1]:.4f}")

        else:
            raise ValueError(f"Unknown optimizer: {optimizer}")
        return losses

"""4:Train Models with Different Optimizers"""

# Initialize models for each optimizer
model_gd = LogisticRegression()
model_bgd = LogisticRegression()
model_sgd = LogisticRegression()
model_mbgd = LogisticRegression()

# Train models
losses_gd = model_gd.fit(X_train, y_train, optimizer='gd', learning_rate=0.01, epochs=1000, verbose=True)
losses_bgd = model_bgd.fit(X_train, y_train, optimizer='bgd', learning_rate=0.01, epochs=1000, verbose=True)
losses_sgd = model_sgd.fit(X_train, y_train, optimizer='sgd', learning_rate=0.01, epochs=1000, verbose=True)
losses_mbgd = model_mbgd.fit(X_train, y_train, optimizer='mbgd', learning_rate=0.01, epochs=1000, batch_size=32, verbose=True)

"""5:Evaluate Models"""

def evaluate(y_true, y_pred):
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    conf_matrix = confusion_matrix(y_true, y_pred)
    return accuracy, precision, recall, f1, conf_matrix

# Predict and evaluate
metrics_gd = evaluate(y_test, model_gd.predict(X_test))
metrics_bgd = evaluate(y_test, model_bgd.predict(X_test))
metrics_sgd = evaluate(y_test, model_sgd.predict(X_test))
metrics_mbgd = evaluate(y_test, model_mbgd.predict(X_test))

# Print results
optimizers = ['Gradient Descent', 'Batch Gradient Descent', 'Stochastic Gradient Descent', 'Mini-Batch Gradient Descent']
for i, metrics in enumerate([metrics_gd, metrics_bgd, metrics_sgd, metrics_mbgd]):
    print(f"\n{optimizers[i]} Metrics:")
    print(f"Accuracy: {metrics[0]:.4f}")
    print(f"Precision: {metrics[1]:.4f}")
    print(f"Recall: {metrics[2]:.4f}")
    print(f"F1 Score: {metrics[3]:.4f}")
    print("Confusion Matrix:\n", metrics[4])

"""6:Plot Loss Curves"""

plt.figure(figsize=(12, 6))
plt.plot(losses_gd, label='Gradient Descent')
plt.plot(losses_bgd, label='Batch Gradient Descent', linestyle='--')
plt.plot(losses_sgd, label='Stochastic Gradient Descent', linestyle='-.')
plt.plot(losses_mbgd, label='Mini-Batch Gradient Descent', linestyle=':')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Loss vs. Epochs')
plt.legend()
plt.show()